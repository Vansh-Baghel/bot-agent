# AI Live Chat Support Agent

A production-ready AI-powered live chat application for customer support.

---

## ğŸš€ Running Locally

### Prerequisites

- Node.js (>= 18)
- PostgreSQL
- A Groq API key ([get one free](https://console.groq.com))

### Backend Setup

1. **Install dependencies**
   ```bash
   cd backend
   npm install
   ```

2. **Configure environment variables**
   
   Create `.env` in `backend/`:
   ```bash
   DATABASE_URL=postgresql://user:password@localhost:5432/spur_chat
   GROQ_API_KEY=your_groq_api_key_here
   PORT=4000
   ```

3. **Set up database**
   ```bash
   # Create database
   createdb spur_chat
   
   # Run migrations
   psql "$DATABASE_URL" -f prisma/schema.sql
   ```

4. **Start backend**
   ```bash
   npm run dev
   ```
   Backend runs at `http://localhost:4000`

### Frontend Setup

1. **Install dependencies**
   ```bash
   cd frontend
   npm install
   ```

2. **Configure environment variables**
   
   Create `.env` in `frontend/`:
   ```bash
   VITE_API_URL=http://localhost:4000
   ```

3. **Start frontend**
   ```bash
   npm run dev
   ```
   Frontend runs at `http://localhost:5173`

---

## ğŸ— Architecture Overview

### Backend Structure

**Layers:**
- **Routes** â€“ HTTP request handling, input validation, response shaping
- **Services** â€“ Business logic (LLM integration isolated in `llm.service.ts`)
- **Persistence** â€“ PostgreSQL as source of truth, Redis for optional caching

**Key principles:**
- Backend owns conversation state
- Frontend never sends chat history to LLM
- One API call per user message
- Clean separation of concerns

### Frontend Structure 

- React state manages UI
- Backend manages data consistency
- Session ID persisted in `localStorage`
- Chat history restored on page reload
- Sidebar updates optimistically

### API Endpoints

- `POST /chat/message` â€“ Send a message (creates/continues session)
- `GET /chat/history/:sessionId` â€“ Fetch conversation history
- `GET /chat/conversations` â€“ Fetch all conversations (sidebar)

---

## ğŸ¤– LLM Integration

### Provider: Groq

**Why Groq?**
- Free tier available
- Extremely fast inference (~300 tokens/sec)
- Simple SDK
- Good response quality

**Model:** `llama-3.1-8b-instant`

### Prompting Strategy

System prompt injects domain knowledge:

```
You are a helpful support agent for a small e-commerce store.

Store policies:
- Shipping: Worldwide shipping, 5â€“7 business days
- Returns: 30-day return window, unused items only
- Support hours: Monâ€“Fri, 9amâ€“6pm IST
```

Each request includes:
- System prompt
- Last N messages from conversation
- Current user message

---

## âš–ï¸ Trade-offs & If I Had More Time

### Current Trade-offs

- **No streaming responses** â€“ Using simple HTTP instead of SSE/WebSockets
- **No authentication** â€“ Single-user mode for simplicity
- **Basic prompting** â€“ No RAG, embeddings, or knowledge base

### If I Had More Time

- **Streaming responses** via Server-Sent Events
- **Authentication** â€“ User accounts + session management
- **Conversation summarization** â€“ For long chat histories
- **Delete/rename chats** â€“ Better conversation management
- **Analytics** â€“ Track response times, user satisfaction
- **RAG integration** â€“ Connect to knowledge base/docs

---

## ğŸ›¡ Error Handling

- Empty messages rejected
- LLM failures handled gracefully
- Backend never crashes on bad input
- Redis failure falls back to PostgreSQL
- No secrets committed to git